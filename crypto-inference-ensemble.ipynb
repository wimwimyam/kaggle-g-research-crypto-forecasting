{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264ed2a0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-01T11:19:44.682552Z",
     "iopub.status.busy": "2022-02-01T11:19:44.681860Z",
     "iopub.status.idle": "2022-02-01T11:19:44.709266Z",
     "shell.execute_reply": "2022-02-01T11:19:44.709729Z",
     "shell.execute_reply.started": "2022-02-01T06:50:50.862433Z"
    },
    "papermill": {
     "duration": 0.050107,
     "end_time": "2022-02-01T11:19:44.709975",
     "exception": false,
     "start_time": "2022-02-01T11:19:44.659868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gresearch_crypto\n",
    "env = gresearch_crypto.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6530e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:19:44.733221Z",
     "iopub.status.busy": "2022-02-01T11:19:44.732611Z",
     "iopub.status.idle": "2022-02-01T11:19:50.727856Z",
     "shell.execute_reply": "2022-02-01T11:19:50.728318Z",
     "shell.execute_reply.started": "2022-02-01T06:50:50.912002Z"
    },
    "papermill": {
     "duration": 6.008597,
     "end_time": "2022-02-01T11:19:50.728487",
     "exception": false,
     "start_time": "2022-02-01T11:19:44.719890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/g-research-crypto-forecasting/supplemental_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8cfd1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:19:50.749559Z",
     "iopub.status.busy": "2022-02-01T11:19:50.748979Z",
     "iopub.status.idle": "2022-02-01T11:19:50.752017Z",
     "shell.execute_reply": "2022-02-01T11:19:50.751530Z",
     "shell.execute_reply.started": "2022-02-01T06:50:57.001778Z"
    },
    "papermill": {
     "duration": 0.014535,
     "end_time": "2022-02-01T11:19:50.752134",
     "exception": false,
     "start_time": "2022-02-01T11:19:50.737599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df =  pd.read_csv(\"../input/g-research-crypto-forecasting/train.csv\")\n",
    "# df = df[pd.to_datetime(df['timestamp'],unit='s')<pd.to_datetime('2021-06-13')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4a9f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:19:50.775466Z",
     "iopub.status.busy": "2022-02-01T11:19:50.774932Z",
     "iopub.status.idle": "2022-02-01T11:19:50.778504Z",
     "shell.execute_reply": "2022-02-01T11:19:50.778063Z",
     "shell.execute_reply.started": "2022-02-01T06:50:57.007115Z"
    },
    "papermill": {
     "duration": 0.01814,
     "end_time": "2022-02-01T11:19:50.778615",
     "exception": false,
     "start_time": "2022-02-01T11:19:50.760475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['Asset_ID','Count', 'Open', 'High', 'Low', 'Close',\n",
    "           'Volume', 'VWAP','time_sin','time_cos']\n",
    "\n",
    "def preprocessing(df):\n",
    "    time = pd.to_datetime(df['timestamp'],unit='s')\n",
    "    time = time.dt.hour * 60 + time.dt.minute\n",
    "    df['time_sin'] = np.sin(2*np.pi*time/1440)\n",
    "    df['time_cos'] = np.cos(2*np.pi*time/1440)\n",
    "    return df\n",
    "\n",
    "def remove_old_data(df,max_len=120):\n",
    "    latest_ts = df['timestamp'].iloc[-1]\n",
    "    mask = df['timestamp'] > (latest_ts-max_len*60)\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130facba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:19:50.806510Z",
     "iopub.status.busy": "2022-02-01T11:19:50.801539Z",
     "iopub.status.idle": "2022-02-01T11:19:50.829154Z",
     "shell.execute_reply": "2022-02-01T11:19:50.828684Z",
     "shell.execute_reply.started": "2022-02-01T06:50:57.019706Z"
    },
    "papermill": {
     "duration": 0.042403,
     "end_time": "2022-02-01T11:19:50.829278",
     "exception": false,
     "start_time": "2022-02-01T11:19:50.786875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = remove_old_data(df)\n",
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a78a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:19:50.851489Z",
     "iopub.status.busy": "2022-02-01T11:19:50.850638Z",
     "iopub.status.idle": "2022-02-01T11:20:19.461382Z",
     "shell.execute_reply": "2022-02-01T11:20:19.460183Z",
     "shell.execute_reply.started": "2022-02-01T06:50:57.061507Z"
    },
    "papermill": {
     "duration": 28.623631,
     "end_time": "2022-02-01T11:20:19.461558",
     "exception": false,
     "start_time": "2022-02-01T11:19:50.837927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/einops-030/einops-0.3.0-py2.py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/einops-030/einops-0.3.0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3692811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:20:19.505225Z",
     "iopub.status.busy": "2022-02-01T11:20:19.504403Z",
     "iopub.status.idle": "2022-02-01T11:20:21.011761Z",
     "shell.execute_reply": "2022-02-01T11:20:21.011241Z",
     "shell.execute_reply.started": "2022-02-01T06:51:25.659836Z"
    },
    "papermill": {
     "duration": 1.53475,
     "end_time": "2022-02-01T11:20:21.011966",
     "exception": false,
     "start_time": "2022-02-01T11:20:19.477216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4bdbe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:20:21.069381Z",
     "iopub.status.busy": "2022-02-01T11:20:21.044362Z",
     "iopub.status.idle": "2022-02-01T11:20:21.071534Z",
     "shell.execute_reply": "2022-02-01T11:20:21.071126Z",
     "shell.execute_reply.started": "2022-02-01T06:51:27.153636Z"
    },
    "papermill": {
     "duration": 0.049756,
     "end_time": "2022-02-01T11:20:21.071642",
     "exception": false,
     "start_time": "2022-02-01T11:20:21.021886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        max_pos_emb = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads= heads\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.rel_pos_emb = nn.Parameter(torch.zeros([1,heads,max_pos_emb,max_pos_emb]))\n",
    "        self.scale = nn.Parameter(torch.full([1,heads,max_pos_emb,max_pos_emb] ,dim_head ** -0.5))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context = None, mask = None, context_mask = None):\n",
    "        n, device, h, has_context = x.shape[-2], x.device, self.heads, exists(context)\n",
    "        context = default(context, x)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "        dots = dots * self.scale + self.rel_pos_emb\n",
    "\n",
    "        if exists(mask) or exists(context_mask):\n",
    "            mask = default(mask, lambda: torch.ones(*x.shape[:2], device = device))\n",
    "            context_mask = default(context_mask, mask) if not has_context else default(context_mask, lambda: torch.ones(*context.shape[:2], device = device))\n",
    "            mask_value = -torch.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, 'b i -> b () i ()') * rearrange(context_mask, 'b j -> b () () j')\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        attn = dots.softmax(dim = -1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "class TransformerLayer2d(nn.Module):\n",
    "    def __init__(self, d_model, nheads, d_hidden, dropout=0.0, nts=90, nasset=14):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.mlp1 = nn.Sequential(nn.Linear(d_ts,d_hidden),nn.GELU(),nn.Dropout(dropout),\n",
    "#                                   nn.Linear(d_hidden,d_ts),nn.Dropout(dropout))\n",
    "        self.attn1 = Attention(d_model, nheads, d_model//nheads, dropout, max_pos_emb=nts)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.mlp2 = nn.Sequential(nn.Linear(d_asset,d_hidden),nn.GELU(),nn.Dropout(dropout),\n",
    "#                                   nn.Linear(d_hidden,d_asset),nn.Dropout(dropout))\n",
    "        self.attn2 = Attention(d_model, nheads, d_model//nheads, dropout, max_pos_emb=nasset)\n",
    "        \n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.mlp3 = nn.Sequential(nn.Linear(d_model,d_hidden),nn.GELU(),nn.Dropout(dropout),\n",
    "                                  nn.Linear(d_hidden,d_model),nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        #(B,ts,asset,D)\n",
    "        B,T,A,D = x.shape\n",
    "        \n",
    "        x2 = self.norm1(x)\n",
    "        x2 = rearrange(x2, 'b t a d -> (b a) t d')\n",
    "        x2 = self.attn1(x2)\n",
    "        x2 = rearrange(x2, '(b a) t d -> b t a d', b=B)\n",
    "        x = x + x2\n",
    "        \n",
    "        x2 = self.norm2(x)\n",
    "        x2 = rearrange(x2, 'b t a d -> (b t) a d')\n",
    "        x2 = self.attn2(x2)\n",
    "        x2 = rearrange(x2, '(b t) a d -> b t a d', b=B)\n",
    "        x = x + x2\n",
    "        \n",
    "        x2 = self.norm3(x)\n",
    "        x2 = self.mlp3(x2)\n",
    "        x = x + x2\n",
    "        return x\n",
    "\n",
    "class TransformerLayer1d(nn.Module):\n",
    "    def __init__(self, d_model, nheads, d_hidden, dropout=0.0, nasset=14):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn1 = Attention(d_model, nheads, d_model//nheads, dropout, max_pos_emb=nasset)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.mlp2 = nn.Sequential(nn.Linear(d_model,d_hidden),nn.GELU(),nn.Dropout(dropout),\n",
    "                                  nn.Linear(d_hidden,d_model),nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        #(B,asset,D)\n",
    "        x2 = self.norm1(x)\n",
    "        x2 = self.attn1(x2)\n",
    "        x = x + x2\n",
    "        \n",
    "        x2 = self.norm2(x)\n",
    "        x2 = self.mlp2(x2)\n",
    "        x = x + x2\n",
    "        return x\n",
    "    \n",
    "class CryptoModel(nn.Module):\n",
    "    def __init__(self, SEQ_LENGTH=60):\n",
    "        super().__init__()\n",
    "        d_model = 64\n",
    "        n_layers = 2\n",
    "        nheads = 4\n",
    "        self.norm = nn.InstanceNorm1d(7)\n",
    "        self.asset_emb = nn.Embedding(14,d_model)\n",
    "        self.dense_emb = nn.Linear(9,d_model)\n",
    "        self.encoder1 = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            self.encoder1.append(TransformerLayer2d(d_model,nheads,4*d_model,0.2,SEQ_LENGTH,14))\n",
    "        self.post_norm1 = nn.LayerNorm(d_model)\n",
    "        self.decoder1 = nn.Sequential(nn.Linear(SEQ_LENGTH,d_model),nn.GELU(),nn.Dropout(0.2),\n",
    "                                  nn.Linear(d_model,1))\n",
    "        self.encoder2 = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            self.encoder2.append(TransformerLayer1d(d_model,nheads,4*d_model,0.2,14))\n",
    "        self.fc = nn.Sequential(nn.Linear(d_model,d_model),nn.GELU(),nn.Dropout(0.0),\n",
    "                                  nn.Linear(d_model,1))\n",
    "        self.out_norm = nn.BatchNorm1d(1, eps=0, affine=False)\n",
    "        \n",
    "        self.pe1 = nn.Parameter(torch.empty([1,SEQ_LENGTH,1,d_model]))\n",
    "        self.pe2 = nn.Parameter(torch.empty([1,1,14,d_model]))\n",
    "        nn.init.normal_(self.pe1, 0.0, 0.2)\n",
    "        nn.init.normal_(self.pe2, 0.0, 0.2)\n",
    "        \n",
    "        self.missing_emb = nn.Parameter(torch.empty([1,SEQ_LENGTH,14,d_model]))\n",
    "        nn.init.normal_(self.missing_emb)\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        inp = inp.clone()\n",
    "        B,T,A,D = inp.shape\n",
    "        mask = inp.abs().sum(-1) == 0\n",
    "        inp = rearrange(inp,'b t a d -> (b a) d t')\n",
    "        inp[:,1:8] = self.norm(inp[:,1:8])\n",
    "        inp = rearrange(inp,'(b a) d t -> b t a d', b=B)\n",
    "        X = self.asset_emb(inp[:,:,:,0].long()) + self.dense_emb(inp[:,:,:,1:])\n",
    "        X[mask] = self.missing_emb.repeat([len(X),1,1,1])[mask]\n",
    "            \n",
    "        X = X + self.pe1 + self.pe2\n",
    "        for layer in self.encoder1:\n",
    "            X = layer(X)\n",
    "        X = self.post_norm1(X)\n",
    "        X = rearrange(X,'b t a d -> b a d t')\n",
    "        X = self.decoder1(X).squeeze(-1)\n",
    "        for layer in self.encoder2:\n",
    "            X = layer(X)\n",
    "        y = self.fc(X).squeeze(-1)\n",
    "        y = self.out_norm(y.unsqueeze(1)).squeeze(1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "234c7631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:20:21.104381Z",
     "iopub.status.busy": "2022-02-01T11:20:21.103735Z",
     "iopub.status.idle": "2022-02-01T11:20:25.151141Z",
     "shell.execute_reply": "2022-02-01T11:20:25.150614Z",
     "shell.execute_reply.started": "2022-02-01T06:51:27.193669Z"
    },
    "papermill": {
     "duration": 4.070321,
     "end_time": "2022-02-01T11:20:25.151270",
     "exception": false,
     "start_time": "2022-02-01T11:20:21.080949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model_45_0 = CryptoModel(45).to(device)\n",
    "model_45_0.eval()\n",
    "model_45_0.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq45-holdout-v4/model_fold0.pt\",map_location=device))\n",
    "\n",
    "model_45_1 = CryptoModel(45).to(device)\n",
    "model_45_1.eval()\n",
    "model_45_1.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq45-holdout-v4/model_fold1.pt\",map_location=device))\n",
    "\n",
    "model_45_2 = CryptoModel(45).to(device)\n",
    "model_45_2.eval()\n",
    "model_45_2.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq45-holdout-v4/model_fold2.pt\",map_location=device))\n",
    "\n",
    "model_60_0 = CryptoModel(60).to(device)\n",
    "model_60_0.eval()\n",
    "model_60_0.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq60-holdout-v4/model_fold0.pt\",map_location=device))\n",
    "\n",
    "model_60_1 = CryptoModel(60).to(device)\n",
    "model_60_1.eval()\n",
    "model_60_1.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq60-holdout-v4/model_fold1.pt\",map_location=device))\n",
    "\n",
    "model_60_2 = CryptoModel(60).to(device)\n",
    "model_60_2.eval()\n",
    "model_60_2.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq60-holdout-v4/model_fold2.pt\",map_location=device))\n",
    "\n",
    "model_90_0 = CryptoModel(90).to(device)\n",
    "model_90_0.eval()\n",
    "model_90_0.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq90-holdout-v4/model_fold0.pt\",map_location=device))\n",
    "\n",
    "model_90_1 = CryptoModel(90).to(device)\n",
    "model_90_1.eval()\n",
    "model_90_1.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq90-holdout-v4/model_fold1.pt\",map_location=device))\n",
    "\n",
    "model_90_2 = CryptoModel(90).to(device)\n",
    "model_90_2.eval()\n",
    "model_90_2.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq90-holdout-v4/model_fold2.pt\",map_location=device))\n",
    "\n",
    "model_120_0 = CryptoModel(120).to(device)\n",
    "model_120_0.eval()\n",
    "model_120_0.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq120-holdout-v4/model_fold0.pt\",map_location=device))\n",
    "\n",
    "model_120_1 = CryptoModel(120).to(device)\n",
    "model_120_1.eval()\n",
    "model_120_1.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq120-holdout-v4/model_fold1.pt\",map_location=device))\n",
    "\n",
    "model_120_2 = CryptoModel(120).to(device)\n",
    "model_120_2.eval()\n",
    "model_120_2.load_state_dict(torch.load(\"../input/cvonly-transformer-freq5-seq120-holdout-v4/model_fold2.pt\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55d3d986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T11:20:25.186622Z",
     "iopub.status.busy": "2022-02-01T11:20:25.179250Z",
     "iopub.status.idle": "2022-02-01T11:20:26.275242Z",
     "shell.execute_reply": "2022-02-01T11:20:26.275668Z",
     "shell.execute_reply.started": "2022-02-01T06:51:30.937545Z"
    },
    "papermill": {
     "duration": 1.114478,
     "end_time": "2022-02-01T11:20:26.275853",
     "exception": false,
     "start_time": "2022-02-01T11:20:25.161375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (df_test, df_pred) in enumerate(iter_test):\n",
    "        current_t = df_test['timestamp'].iloc[0]\n",
    "        row_order = df_test['Asset_ID'].values\n",
    "        df_test = preprocessing(df_test)\n",
    "        df = pd.concat([df,df_test],axis=0)\n",
    "        df = remove_old_data(df)\n",
    "        X = np.zeros([120,14,10])\n",
    "        for t, d in df.groupby('timestamp'):\n",
    "            t = (t - (current_t - 120*60))//60 - 1\n",
    "            if (t < 0) or (t >= 120):\n",
    "                continue\n",
    "            X[t,d['Asset_ID'].values] = d[features].values\n",
    "\n",
    "        X = X.astype(np.float32)\n",
    "        X[:,:,-4] = np.log1p(X[:,:,-4])\n",
    "        X[:,:,-3] = np.log1p(X[:,:,-3])\n",
    "        X[np.isnan(X)|np.isinf(X)] = 0\n",
    "\n",
    "        X = torch.Tensor(X).to(device).unsqueeze(0)\n",
    "\n",
    "        pred = np.zeros(14)\n",
    "        pred += model_45_0(X[:,-45:])[0].cpu().numpy()\n",
    "        pred += model_45_1(X[:,-45:])[0].cpu().numpy()\n",
    "        pred += model_45_2(X[:,-45:])[0].cpu().numpy()\n",
    "        pred += model_60_0(X[:,-60:])[0].cpu().numpy()\n",
    "        pred += model_60_1(X[:,-60:])[0].cpu().numpy()\n",
    "        pred += model_60_2(X[:,-60:])[0].cpu().numpy()\n",
    "        pred += model_90_0(X[:,-90:])[0].cpu().numpy()\n",
    "        pred += model_90_1(X[:,-90:])[0].cpu().numpy()\n",
    "        pred += model_90_2(X[:,-90:])[0].cpu().numpy()\n",
    "        pred += model_120_0(X)[0].cpu().numpy()\n",
    "        pred += model_120_1(X)[0].cpu().numpy()\n",
    "        pred += model_120_2(X)[0].cpu().numpy()\n",
    "        pred /= 12\n",
    "        \n",
    "        df_pred['Target'] = pred[row_order]\n",
    "        # Send submissions\n",
    "        env.predict(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617fc79",
   "metadata": {
    "papermill": {
     "duration": 0.00955,
     "end_time": "2022-02-01T11:20:26.295440",
     "exception": false,
     "start_time": "2022-02-01T11:20:26.285890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.690562,
   "end_time": "2022-02-01T11:20:27.215398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-01T11:19:36.524836",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
